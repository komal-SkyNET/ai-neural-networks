{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_dqn_impl_lunar_lander.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjFaZuFfbesCVpQqId/5VZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komal-SkyNET/ai-neural-networks/blob/master/google-colab/tf2_dqn_impl_lunar_lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C4WbyNx1bF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install gym[box2d]\n",
        "!pip3 install box2d-py\n",
        "# !pip3 install 'gym[all]'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j590CQQ881lZ",
        "colab_type": "code",
        "outputId": "cc8acfe4-3e9a-4c10-fd2f-0ffe6ab27fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUqfHV2R97wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, mem_size, input_dims):\n",
        "    self.mem_size = mem_size\n",
        "    self.mem_cntr = 0\n",
        "    self.state_mem = np.zeros((self.mem_size, *input_dims), \n",
        "                                 dtype=np.float32)\n",
        "    self.new_state_mem = np.zeros((self.mem_size, *input_dims), \n",
        "                                 dtype=np.float32)\n",
        "    self.action_mem = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.reward_mem = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.terminal_mem = np.zeros(self.mem_size, dtype=np.int32)\n",
        "\n",
        "  def store_transition(self, state, action, rew, state_, done):\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.state_mem[index] = state\n",
        "    self.action_mem[index] = action\n",
        "    self.new_state_mem[index] = state_\n",
        "    self.reward_mem[index] = reward\n",
        "    self.terminal_mem[index] = 1 - int(done)\n",
        "    self.mem_cntr += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "    states = self.state_mem[batch]\n",
        "    states_ = self.new_state_mem[batch]\n",
        "    rewards = self.reward_mem[batch]\n",
        "    actions = self.action_mem[batch]\n",
        "    terminal = self.terminal_mem[batch]\n",
        "\n",
        "    return states, actions, rewards, states_, terminal\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7qA8dzl02F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Dense(fc1_dims, input_shape = input_dims, \n",
        "                       activation='relu'),\n",
        "    keras.layers.Dense(fc2_dims, activation='relu'),\n",
        "    keras.layers.Dense(n_actions, activation=None)])\n",
        "  model.compile(optimizer=Adam(learning_rate=lr),\n",
        "                loss='mean_squared_error')\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xdWcrnEnLPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "  \n",
        "  def __init__(self, lr, gamma, n_actions, epsilon, batch_size, \n",
        "               input_dims, epsilon_dec=1e-4, epsilon_end=0.01,\n",
        "               mem_size = 1000000, fname='dqn_model.pkl'):\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.gamma = gamma\n",
        "    self.eps_dec = epsilon_dec\n",
        "    self.epsilon = epsilon \n",
        "    self.eps_min = epsilon_end\n",
        "    self.batch_size = batch_size\n",
        "    self.model_file = fname\n",
        "    self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "    self.q_eval = build_dqn(lr, n_actions, input_dims, 256, 256)\n",
        "\n",
        "  def store_transition(self, state, action, reward, new_state, done):\n",
        "    self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "  def choose_action(self, observation):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = np.random.choice(self.action_space)\n",
        "    else:\n",
        "      # if 8 observations , input dims need it in (1,8) dims\n",
        "      state = np.array([observation])\n",
        "      actions = self.q_eval.predict(state)\n",
        "      action = np.argmax(actions)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_cntr < self.batch_size:\n",
        "      return \n",
        "\n",
        "    states, actions, rewards, states_, dones = \\\n",
        "          self.memory.sample_buffer(self.batch_size)\n",
        "    \n",
        "    q_eval = self.q_eval.predict(states)\n",
        "    q_next = self.q_eval.predict(states_)\n",
        "\n",
        "    q_target = np.copy(q_eval)\n",
        "    batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    q_target[batch_index, actions] = rewards + \\\n",
        "          self.gamma * np.max(q_next, axis=1)*dones\n",
        "    self.q_eval.train_on_batch(states, q_target)\n",
        "    self.epsilon = self.epsilon - self.eps_dec if self.epsilon > \\\n",
        "        self.eps_min else self.eps_min\n",
        "\n",
        "  def save_model(self):\n",
        "    self.q_eval.save(self.model_file)\n",
        "  \n",
        "  def load_model(self):\n",
        "    self.q_eval = load_model(self.model_file)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sEcLaM-vxTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Driver\n",
        "import gym \n",
        "\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "env = gym.make('LunarLander-v2')\n",
        "n_games = 1000\n",
        "lr = 0.001\n",
        "print(env.observation_space.shape)\n",
        "print(env.action_space.n)\n",
        "agent = Agent(gamma = 0.99, epsilon=1.0, lr = lr,\n",
        "              input_dims=env.observation_space.shape, \n",
        "              n_actions = env.action_space.n, \n",
        "              mem_size = 1000000, batch_size=64,\n",
        "              epsilon_end=0.01)\n",
        "\n",
        "scores = []\n",
        "eps_history = []\n",
        "\n",
        "for i in range(n_games):\n",
        "  done = False\n",
        "  score = 0\n",
        "  obs = env.reset()\n",
        "  while not done:\n",
        "    action = agent.choose_action(obs)\n",
        "    obs_, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "    agent.store_transition(obs, action, reward, obs_, done)\n",
        "    obs = obs_\n",
        "    agent.learn()\n",
        "  eps_history.append(agent.epsilon)\n",
        "  scores.append(score)\n",
        "\n",
        "  avg_score = np.mean(scores[-100:])\n",
        "  print('episode: ', i , 'score %.2f' % score, \n",
        "        'average_score: %.2f' % avg_score, \n",
        "         'epsilon %.2f' % agent.epsilon)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}